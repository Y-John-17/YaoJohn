<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Big Data on Yao John</title>
    <link>/tags/big-data/</link>
    <description>Recent content in Big Data on Yao John</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/big-data/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hadoop</title>
      <link>/post/2022/11/30/hadoop/</link>
      <pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022/11/30/hadoop/</guid>
      <description>Hadoop-3.1.4 完全分布式集群搭建 Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。Hadoop实现了一个分布式文件系统（ Distributed File System），其中一个组件是HDFS（Hadoop Distributed File System）。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming access）文件系统中的数据。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。
虚拟机安装和网卡启用 虚拟机安装过程掠过，自行百度查阅了解
本文所采用的虚拟机版本为 cetos7
IP地址的设置要确保和你的虚拟网卡网络地址网段相同
主机名 IP地址 备注 node1 192.168.153.151 主节点，数据节点 node2 192.168.153.152 副节点，数据节点 node3 192.168.153.153 数据节点 启用网卡
vi /etc/sysconfig/networscripts/ifcfg-ens33 ONBOOT = yes DNS1 = 8.8.8.8 ping baidu.com # ping通即可 主机名和IP地址映射
在原有文件下添加如下字段 vim /etc/hosts 192.168.153.151 node1 192.168.153.152 node2 192.168.153.153 node3 JAVA 环境部署 Hadoop的创始人是Doug Cutting， 同时也是著名的基于Java的检索引擎库Apache Lucene的创始人。Hadoop本来是用于著名的开源搜索引擎Apache Nutch，而Nutch本身是基于Lucene的，而且也是Lucene的一个子项目。因此Hadoop基于Java就很理所当然了，所以，Hadoop是由Java编写的。
Hadoop采用Java编写，因而Hadoop天生支持Java语言编写作业，但在实际应用中，有时候，因要用到非Java的第三方库或者其他原因，要采用C/C++或者其他语言编写MapReduce作业，这时候可能要用到Hadoop提供的一些工具。
如果你要用C/C++编写MpaReduce作业，可使用的工具有Hadoop Streaming或者Hadoop Pipes。
如果你要用Python编写MapReduce作业，可以使用Hadoop Streaming或者Pydoop。
如果你要使用其他语言，如shell，php，ruby等，可使用Hadoop Streaming。
安装
进入/opt路径创建并进入/software，将jdk-8u281-linux-x64.rpm文件上传到/opt/software cd /opt mkdir software cd software rpm -ivh jdk-8u281-linux-x64.</description>
    </item>
    
    <item>
      <title>Hadoop词频统计</title>
      <link>/post/2022/11/30/hadoop%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1/</link>
      <pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022/11/30/hadoop%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1/</guid>
      <description>Hadoop词频统计 前期准备 该实验采用Hadoop单分布模式即可满足，需要自己前期搭建
Hadoop守护进程启动
cd ./hadoop/sbin/start-all.sh 查看相关守护进程是否已经启动 使用jps命令 Jps NodeManager SecondaryNameNode NameNode DataNode ResourceManager 数据集预处理
数据集编码格式UTF-8,所采用数据集中词频采用统一分隔符，否则需要自行编写函数 在Linux用户目录下新建一个wordcount文件夹，导入数据集 在hdfs新建文件目录 hdfs dfs -mkdir /wordcount hdfs dfs -mkdir /wordcount/input hdfs dfs -mkdir /wordcount/output 将数据集存储到hdfs cd /tmp/wordcount hdfs dfs -put wordcount.txt /wordcount/input hdfs dfs -ls /input 使用Hadoop自带的Jar文件脚本处理数据 数据集处理
cd ./hadoop/share/hadoop/mapreduce/
ll hadoop-mapreduce-examples-3.1.3.jar
hadoop jar ./hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wordcount/input/wordcount.txt wordcount/output #jar表示运行脚本类型，后面是具体脚本存储路径，wordcount是脚本中Java主函数类名，后面是数据集存储路径和输出路径 数据集处理结果查看
hdfs dfs -cat /wordcount/output/part-r-00000 数据集处理结果下载
hdfs dfs -get /wordcount/output/
End ! </description>
    </item>
    
    <item>
      <title>MapReduce气温统计实验</title>
      <link>/post/2022/11/30/mapreduce%E6%B0%94%E6%B8%A9%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C/</link>
      <pubDate>Wed, 30 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022/11/30/mapreduce%E6%B0%94%E6%B8%A9%E7%BB%9F%E8%AE%A1%E5%AE%9E%E9%AA%8C/</guid>
      <description>MapReduce气温统计实验 一、数据集准备 数据集下载地址：ftp://ftp.ncdc.noaa.gov/pub/data/uscrn/products/daily01
将下载下来的数据集合并，更名为 temperature.txt
二、数据集导入 将预处理好的数据导入Linux中，并启动Hadoop集群守护进程
$ ll temperature.txt 查看文件是否导入成功
在hdfs创建文件路径和上传数据集
$ hdfs dfs -mkdir temperature
$ hdfs dfs -mkdir temperature/input
$ hdfs dfs -mkdir temperature/output
$ hdfs dfs -put temperature.txt temperature/input
在浏览器中核验文件是否上传成功（注意关闭防火墙）
192.168.153.147:50070 192.168.153.147:8088
三、数据处理脚本编译 编写 java 文件
$ vim MyMaxMin.java
写入脚本代码
import java.io.IOException; import java.util.Iterator; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.conf.Configuration; public class MyMaxMin { //Mapper /** *MaxTemperatureMapper class is static and extends Mapper abstract class having four hadoop generics type LongWritable, Text, Text, Text.</description>
    </item>
    
  </channel>
</rss>
